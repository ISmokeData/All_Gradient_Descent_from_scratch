# Gradient Descent Algorithms from Scratch

Welcome to the Gradient Descent Algorithms repository! This repository provides implementations of various gradient descent algorithms from scratch. These implementations are aimed at helping you understand the mechanics and nuances of different optimization techniques used in machine learning and deep learning.

## Table of Contents
- Overview
- Implemented Algorithms
- Installation
- Contributing
- License

## Overview

Gradient descent is a fundamental optimization algorithm used to minimize the cost function in various machine learning models. This repository includes a collection of gradient descent algorithms, implemented from scratch without using any external machine learning libraries. The goal is to provide a clear and concise understanding of how these algorithms work internally.

## Implemented Algorithms

1. **Basic Gradient Descent**
2. **Stochastic Gradient Descent (SGD)**
3. **Mini-Batch Gradient Descent**

## Installation

To get started, clone the repository to your local machine:

```bash
git clone https://github.com/ISmokeData/All_Gradient_Descent_from_scratch.git
All_Gradient_Descent_from_scratch
```

You will need Python 3.x and the following library:

- NumPy

You can install the required library using:

```bash
pip install numpy
```

## Contributing

Contributions are welcome! If you have any improvements, suggestions, or additional gradient descent algorithms to add, please feel free to open an issue or submit a pull request. 

When contributing, please follow the standard guidelines:
1. Fork the repository
2. Create a new branch (`git checkout -b feature-branch`)
3. Commit your changes (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature-branch`)
5. Create a new Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
